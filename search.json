[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Групові проєкти з курсу \"Прикладне математичне моделювання в R\"",
    "section": "",
    "text": "Матеріали підготовлені для читання курсу “Вступ до прикладного математичного моделювання в R” [08.435] для студентів 3-го курсу, спеціальності економічна кібернетика.\n\n\nНавчальна дисципліна спрямована на вивчення основ прикладного математичного моделювання з використаням різних видів моделей та підходів до їх побудови, а також удосокналенню процесів роботи з даними та моделями на основі мови R.\nМісце навчальної дисципліни у підготовці здобувачів: програмні результати дисципліни використовуються під час вивчення таких навчальних дисциплін: “Математичні методи та моделі в аналізі великих даних”, “Економічна кібернетика”, “Дослідження операцій”, “Інтелектуальні технології моделювання у прийнятті рішень”. Закріплення на практиці здобутих програмних результатів відбувається під час проходження Навчальної практики з курсу «Економіко-математичне моделювання».\n\n\n\nМета навчальної дисципліни – формування у студентів теоретичних знань та практичних навичок використання мови програмування R для побудови, навчання та оцінки якості математичних моделей на основі регресії, класифікації, кластеризації та асоціативних правил.\n\n\n\nВикладач та слухач цього курсу, як очікується, повинні дотримуватися Кодексу академічної доброчесності університету:\n\nбудь-яка робота, подана здобувачем протягом курсу, має бути його власною роботою здобувача; не вдаватися до кроків, що можуть нечесно покращити Ваші результати чи погіршити/покращити результати інших здобувачів;\nякщо буде виявлено ознаки плагіату або іншої недобросовісної академічної поведінки, то студент буде позбавлений можливості отримати передбачені бали за завдання;\nне публікувати у відкритому доступі відповіді на запитання, що використовуються в рамках курсу для оцінювання знань здобувачів;\nпід час фінальних видів контролю необхідно працювати самостійно; не дозволяється говорити або обговорювати, а також не можна копіювати документи, використовувати електронні засоби отримання інформації.\n\nПорушення академічної доброчесності під час виконання контрольних завдань призведе до втрати балів або вживання заходів, які передбачені Кодексу академічної доброчесності НаУОА.\n\n\n\n\n\n\n\nМатеріали створені з використанням ряду технологій:\n\nR Language - безкоштована мова програмування для виконання досліджень у сфері статистики, машинного навчання та візуалізацї результатів.\nQuarto Book - система для публікації наукових та технічних текстів з відкритим кодом (R/Python/Julia/Observable).\nJupyterLab - середовище розробки на основі Jupyter Notebook. JupyterLab є розширеним веб-інтерфейсом для роботи з ноутбуками.\nGit/Github - система контролю версій та, відповідно, сервіс для організації зберігання коду, а також публікації статичних сторінок."
  },
  {
    "objectID": "02-pca.html",
    "href": "02-pca.html",
    "title": "1  Principal Component Analysis (in R)",
    "section": "",
    "text": "Груповий проєкт з курсу “Прикладне математичне моделювання в R”\nГрупа: ЕК-3.\nФакультет: Економічний факультет.\nРік: 2022"
  },
  {
    "objectID": "02-pca.html#мета-проєкту",
    "href": "02-pca.html#мета-проєкту",
    "title": "1  Principal Component Analysis (in R)",
    "section": "1.1 Мета проєкту",
    "text": "1.1 Мета проєкту\nМетою проєкту є теоретичне вивчення та практичне застосування алгоритмів з прикладного математичного моделювання в R. Додатковими задачами, які ставляться під час виконання проєкту є розвиток навиків самоосвіти та колективної роботи студентів. Основна задача, яка ставиться перед виконуючими - донести теоретичні засади такого методу аналізу, доступні пакети, візуалізувати роботу аналізу та практично показати використання методу."
  },
  {
    "objectID": "02-pca.html#теоретична-частина",
    "href": "02-pca.html#теоретична-частина",
    "title": "1  Principal Component Analysis (in R)",
    "section": "1.2 Теоретична частина",
    "text": "1.2 Теоретична частина\nВступ\nPrincipal Component Analysis або Аналіз головних компонентів (далі РСА) - техніка без вчителя для дослідницького аналізу даних, яка дозволяє краще візуалізувати варіацію, що знаходиться в наборі даних з багатьома змінними. Такий аналіз є дуже корисним у випадку так званих “широких” наборів даних, тобто, коли ми маємо багато змінних для кожної вибірки.\nЗменшення кількості змінних у наборі даних відбувається за рахунок точності, але тут зменшення розмірності полягає в тому, щоб обміняти невелику точність на простоту. Адже менші набори даних легше досліджувати і візуалізувати. Це робить аналіз даних набагато простішим і швидшим для алгоритмів машинного навчання без зайвих змінних для обробки.\nОтже, загальна ідея РСА - зменшити кількість змінних набору даних, зберігши при цьому якомога більше інфомації.\nДе є корисний такий метод?\nМетод корисний в будь-якій сфері, яка працює з великими наборами даних: медицина, наука, розробка і так далі. В таких випадках, коли присутня велика кількість змінних, ми не можемо побудувати графік даних в їхньому необробленому форматі, що дещо ускладнює розуміння досліджуваних тенденцій. РСА ж дозволяє побачити загальний вигляд даних, відмічаючи, які вибірки схожі одна на одну, а які дуже відрізняються. Це дозволяє визначити групи вибірок, які є схожими і з’ясувати, які змінні роблять одну групу відмінною від іншої.\nГоловні компоненти\nГоловні компоненти - це вже нові змінні, які будуються як лінійні комбінації або суміші вихідних змінних. Ці комбінації складаються таким чином, що нові змінні (наші головні компоненти) є некорельованими, а більша частина інформації, що міститься у вихідних змінних, стискається або витискається.\nЯкщо багато змінних корелюють одна з одною, то всі вони будуть вносити значний внесок в одну головну компоненту. Кожна компонента підсумовує певний відсоток загальної варіації в наборі даних. Якщо ваші вихідні змінні сильно корелюють між собою, ви зможете апроксимувати більшу частину складності вашого набору даних лише кількома головними компонентами. Коли ви додаєте більше головних компонент, ви узагальнюєте все більше вихідного набору даних. Додавання додаткових компонентів робить вашу оцінку загального набору даних більш точною, але і більш громіздкою.\nПроцес аналізу\nНа практиці використовується такі кроки в аналізі.\nСтандартизація\nРСА дуже чутлива до варіацій вихідних змінних. Тобто, якщо існують великі відмінності між діапазонами вихідних змінних, то такі змінні з більшими діапазонами будуть домінувати над змінними з малими діапазонами (наприклад, змінна, яка знаходиться в діапазоні від 0 до 10, буде домінуват над змінною, яка знаходиться в діапазоні від 0 до 1), що призведе до упереджених результатів.\nМатематично це можна зробити шляхом віднімання середнього знаечння та ділення та стандартне відхилення для кожного значення кожної змінної.\nОбчислення коваріаційної матриці\nМета цього кроку - зрозуміти, як змінні вхідного набору даних відхиляються від середнього значення по відношенню одна до одної, або, іншими словами, побачити, чи існує між ними зв’язок.Тому що іноді змінні сильно корелюють таким чином, що містять надлишкову інформацію. Так от, для того, щоб виявити ці кореляції, ми обчислюємо коваріаційну матрицю.\nКоваріаційна матриця - це симетрична матриця p × p (де p - кількість вимірів), яка має в якості записів коваріації, пов’язані з усіма можливими парами вихідних змінних.\nЩо коваріації, які ми маємо у вигляді елементів матриці, говорять нам про кореляції між змінними?\nМає значення знак коваріації: - якщо він позитивний, то дві змінні збільшуються або зменшуються разом (корелюють). - якщо від’ємний, то одна збільшується, коли інша зменшується (обернено корельована).\nОбчислення власних весторів та власних значень коваріаційної матриці для виявлення головних компонент\nКожен власний вектор має відповідне власне значення. Простими словами, власний вектор - це напрямок (вертикаль, 50 градусів), власне значення - це число, яке показує, наскільки сильно розрізняються дані в цьому напрямку. В такому випадку, власний вектор з найбільшим власним значенням є першою головною компонентою. Наступний власний вектор з великим власним значенням - друга головна компонента. І так далі.\nВажливо розуміти, що головні компоненти є менш інтерпретованими і не мають реального значення, оскільки вони побудовані як лінійні комбінації вихідних змінних.\nПакети, що використовуються для РСА\n\ntidyverse. Найпростіший пакет для застосування аналізу.\nFactoMineR. Пакет для узагальнення, візуалізації та опису наборів даних. Функція PCA().\nfactoextra для візуалізації та кращої інтерперетації результатів. Він є більш ширшим і використовуваним при дослідженні набору даних з допомогою РСА.\nstats - пакет з статистичними функціями, де знаходяться дуже популярні функції як prcomp() та princomp().\nade4 - пакет з інструментами для багатовимірного аналізу даних з функцією dudi.pca().\namap - пакет з інструментами для кластерзації та аналізу головних компонентів з функцією acp()."
  },
  {
    "objectID": "02-pca.html#практична-частина",
    "href": "02-pca.html#практична-частина",
    "title": "1  Principal Component Analysis (in R)",
    "section": "1.3 Практична частина",
    "text": "1.3 Практична частина\nТут ми розглянемо декілька методів аналізу набору даних за допомогою головних компонентів.\nПакет tidyverse\nТут ми будемо досліджувати набір даних USArrests, що вбудований в R, який містить кількість арештів на 100 000 людей в кожному штаті США в 1973 за вбивство, грабіж та насилля. Також він містить процент популяції, що живе в місті.\nЗавантажуємо пакет та датасет.\n\n# install.packages(\"tidyverse\")\nsuppressMessages(library(tidyverse))\n\n\ndata(\"USArrests\")\nhead(USArrests)\n\n\n\nA data.frame: 6 × 4\n\n    MurderAssaultUrbanPopRape\n    <dbl><int><int><dbl>\n\n\n    Alabama13.22365821.2\n    Alaska10.02634844.5\n    Arizona 8.12948031.0\n    Arkansas 8.81905019.5\n    California 9.02769140.6\n    Colorado 7.92047838.7\n\n\n\n\nПісля завантаження потрібних файлів та інструментів, ми можемо використати функцію prcomp() аби обрахувати головні компоненти набору даних. Обов’язково вкажемо scale = TRUE, щоб кожна змінна в наборі даних була масштабована так, щоб мати середнє значення 0 і стандартне відхилення 1 перед обчисленням головних компонент.\n\npc <- prcomp(USArrests, scale = TRUE)\n\nВласні вектори в R за замовчуванням спрямовані у від’ємному напрямку, тому ми помножимо їх на -1, щоб змінити знаки на протилежні.\n\npc$rotation <- -1*pc$rotation\npc$rotation\npc\n\n\n\nA matrix: 4 × 4 of type dbl\n\n    PC1PC2PC3PC4\n\n\n    Murder0.5358995-0.4181809 0.3412327-0.64922780\n    Assault0.5831836-0.1879856 0.2681484 0.74340748\n    UrbanPop0.2781909 0.8728062 0.3780158-0.13387773\n    Rape0.5434321 0.1673186-0.8177779-0.08902432\n\n\n\n\nStandard deviations (1, .., p=4):\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\nRotation (n x k) = (4 x 4):\n               PC1        PC2        PC3         PC4\nMurder   0.5358995 -0.4181809  0.3412327 -0.64922780\nAssault  0.5831836 -0.1879856  0.2681484  0.74340748\nUrbanPop 0.2781909  0.8728062  0.3780158 -0.13387773\nRape     0.5434321  0.1673186 -0.8177779 -0.08902432\n\n\nБачимо, що перша головна компонента (РС1) має високе значення для вбивств, пограбувань та насильства, що означає, що вона (компонента) описує більшу частину варіації в цих змінних.\nТакож бачимо, що друга головна компонента (РС2) має високе значення в відсотку популяції, що живе в місті. Це означає, що компонента описує більшу частину варіації цієї змінної.\nОцінки головних компонент кожного штату зберігаються в колонці х, тому помножимо ці оцінки на -1, аби змінити знаки на протилежні.\n\npc$x <- -1*pc$x\nhead(pc$x)\npc\n\n\n\nA matrix: 6 × 4 of type dbl\n\n    PC1PC2PC3PC4\n\n\n    Alabama 0.9756604-1.1220012 0.43980366-0.154696581\n    Alaska 1.9305379-1.0624269-2.01950027 0.434175454\n    Arizona 1.7454429 0.7384595-0.05423025 0.826264240\n    Arkansas-0.1399989-1.1085423-0.11342217 0.180973554\n    California 2.4986128 1.5274267-0.59254100 0.338559240\n    Colorado 1.4993407 0.9776297-1.08400162-0.001450164\n\n\n\n\nStandard deviations (1, .., p=4):\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\nRotation (n x k) = (4 x 4):\n               PC1        PC2        PC3         PC4\nMurder   0.5358995 -0.4181809  0.3412327 -0.64922780\nAssault  0.5831836 -0.1879856  0.2681484  0.74340748\nUrbanPop 0.2781909  0.8728062  0.3780158 -0.13387773\nRape     0.5434321  0.1673186 -0.8177779 -0.08902432\n\n\nДалі можемо створити biplot - діаграму, яка проектує кожне спостереження в наборі даних на діаграму розсіювання, яка використовує першу та другу головні компоненти в якості осей.\n\nbiplot(pc, scale = 0)\n\n\n\n\nШтати, що розташовані близько один до одного, мають подібні моделі даних щодо змінних. Також бачимо, що деякі штати більш тісно пов’язані з певними злочинами, ніж інші. Наприклад, Джорджія є найбільш близькою до змінної Murder. Якщо подивимося на штати з найвищим показником вбивств, то побачимо, що Джорджія очолює цей список.\n\nhead(USArrests[order(-USArrests$Murder),])\n\n\n\nA data.frame: 6 × 4\n\n    MurderAssaultUrbanPopRape\n    <dbl><int><int><dbl>\n\n\n    Georgia17.42116025.8\n    Mississippi16.12594417.1\n    Florida15.43358031.9\n    Louisiana15.42496622.2\n    South Carolina14.42794822.5\n    Alabama13.22365821.2\n\n\n\n\nМи можемо використати наступний код для розрахунку загальної дисперсії вихідного набору даних, що пояснюється кожною головною компонентою.\n\npca <- pc$sdev^2 / sum(pc$sdev^2)\npca\n\n\n0.6200603947873740.247441288134960.08914079514520740.0433575219324588\n\n\nЗ результатів бачимо, що:\n\nперша головна компонента пояснює 62% загальної варіації набору даних.\nдруга головна компонента пояснює 24.7% загальної варіації набору даних.\nтретя головна компонента пояснює 8.9% загальної варіації набору даних.\nчетверта головна компонента пояснює 4.3% загальної варіації набору даних.\n\nТобто, перші 2 головні компоненти описують добре майже весь датасет. Створимо графік для відображення всіх компонент.\n\nqplot(c(1:4), pca) + \n  geom_line() + \n  xlab(\"Головна компонента\") + \n  ylab(\"Пояснення варіації\")\n\n\n\n\nПеревіримо кореляцію головних компонент зі змінними в наборі даних.\n\nUS <- cbind(USArrests, pc$x[,1:2])\nhead(US)\ncor(USArrests, US[,5:6])\n\n\n\nA data.frame: 6 × 6\n\n    MurderAssaultUrbanPopRapePC1PC2\n    <dbl><int><int><dbl><dbl><dbl>\n\n\n    Alabama13.22365821.2 0.9756604-1.1220012\n    Alaska10.02634844.5 1.9305379-1.0624269\n    Arizona 8.12948031.0 1.7454429 0.7384595\n    Arkansas 8.81905019.5-0.1399989-1.1085423\n    California 9.02769140.6 2.4986128 1.5274267\n    Colorado 7.92047838.7 1.4993407 0.9776297\n\n\n\n\n\n\nA matrix: 4 × 2 of type dbl\n\n    PC1PC2\n\n\n    Murder0.8439764-0.4160354\n    Assault0.9184432-0.1870211\n    UrbanPop0.4381168 0.8683282\n    Rape0.8558394 0.1664602\n\n\n\n\nМи можемо надалі використовувати перші 2 компоненти для досліджень. Це значно спрощує роботу, адже тепер ми маємо лише 2 змінні, що включають в себе більшу частину датасету.\nПакет ade4() з функцією dudi.pca()\n\n#install.packages(\"ade4\")\nlibrary(ade4)\n\n\npc1 <- dudi.pca(USArrests, nf = 5, scannf = FALSE)\npc1$eig\n\n\n2.480241579149490.9897651525398390.356563180580830.173430087729835\n\n\n\npc1$co\n\n\n\nA data.frame: 4 × 4\n\n    Comp1Comp2Comp3Comp4\n    <dbl><dbl><dbl><dbl>\n\n\n    Murder-0.8439764 0.4160354-0.2037600 0.27037052\n    Assault-0.9184432 0.1870211-0.1601192-0.30959159\n    UrbanPop-0.4381168-0.8683282-0.2257242 0.05575330\n    Rape-0.8558394-0.1664602 0.4883190 0.03707412\n\n\n\n\n\nhead(pc1$li)\n\n\n\nA data.frame: 6 × 4\n\n    Axis1Axis2Axis3Axis4\n    <dbl><dbl><dbl><dbl>\n\n\n    Alabama-0.9855659 1.1333924-0.44426879 0.156267145\n    Alaska-1.9501378 1.0732133 2.04000333-0.438583440\n    Arizona-1.7631635-0.7459568 0.05478082-0.834652924\n    Arkansas 0.1414203 1.1197968 0.11457369-0.182810896\n    California-2.5239801-1.5429340 0.59855680-0.341996478\n    Colorado-1.5145629-0.9875551 1.09500699 0.001464887\n\n\n\n\n\nlibrary(ggplot2)\nscores <- as.data.frame(pc1$li)\n\n\nggplot(data = scores, aes(x = Axis1, y = Axis2, label = rownames(scores))) +\n  geom_hline(yintercept = 0, colour = \"gray65\") +\n  geom_vline(xintercept = 0, colour = \"gray65\") +\n  geom_text(colour = \"red\", alpha = 0.8, size = 4) +\n  ggtitle(\"PCA графік\")"
  },
  {
    "objectID": "03-сlustering.html",
    "href": "03-сlustering.html",
    "title": "2  Clustering: теорія та пакети R",
    "section": "",
    "text": "Груповий проєкт з курсу “Прикладне математичне моделювання в програмуванні”\nІнформація про виконавців\nПІБ: Архипова Анастасія Ігорівна та Палієнко Ірина Миколаївна.\nКурс: 3.\nСпеціальність: Економічна кібернетика.\nКафедра: кафедра економіко-математичного моделювання та інформаційних технологій.\nФакультет: Економічний факультет Національного університету “Острозька академія”."
  },
  {
    "objectID": "03-сlustering.html#мета-проєкту",
    "href": "03-сlustering.html#мета-проєкту",
    "title": "2  Clustering: теорія та пакети R",
    "section": "2.1 Мета проєкту",
    "text": "2.1 Мета проєкту\nМетою проєкту є теоретичне вивчення та практичне застосування алгоритмів з прикладного математичного моделювання в R. Також розвиток теоретичних та практичних навичок по темі клатеризування за допомогою програмного середовища R."
  },
  {
    "objectID": "03-сlustering.html#теоретична-частина",
    "href": "03-сlustering.html#теоретична-частина",
    "title": "2  Clustering: теорія та пакети R",
    "section": "2.2 Теоретична частина",
    "text": "2.2 Теоретична частина\nКластерний аналіз\nЦе статистичний метод обробки даних. Він працює, організовуючи елементи в групи або кластери на основі того, наскільки тісно вони пов’язані. Коротко кажучи, це один з методів класифікації, який використовуються в машинному навчанні.\nУ кластеризації або кластерному аналізі в R ми намагаємося згрупувати об’єкти зі схожими ознаками та особливостями, щоб більший набір об’єктів розділився на менші.\nНайпоширенішим використанням кластерного аналізу є класифікація. Суб’єкти поділяються на групи так, щоб кожен суб’єкт був більш схожий на інші суб’єкти у своїй групі, ніж на ті, що поза групою.\n\nДе можна використовувати кластерний аналіз\nУ контексті дослідження ринку це можна використовувати для визначення таких категорій, як вікові групи, розміри доходів, міське, сільське чи приміське розташування.\n\nУ маркетингу кластерний аналіз можна використовувати для сегментації аудиторії, щоб на різні групи клієнтів можна було націлити найбільш відповідні повідомлення.\nДослідники в галузі охорони здоров’я можуть використовувати кластерний аналіз, щоб з’ясувати, чи пов’язані різні географічні області з високим або низьким рівнем певних захворювань, щоб вони могли досліджувати можливі місцеві фактори, що сприяють проблемам зі здоров’ям.\nІдентифікація фейкових новин. Алгоритми кластеризації допомагають зіставляти новини, факти та поради з перевіреними джерелами та класифікувати їх як правду, напівправду та брехню.\nСпам-фільтри. Спам-фільтри є класичним прикладом класифікаційних моделей. Вони класифікують електронні листи та повідомлення як важливі та спам на основі вмісту всередині них.\n\nЯкою б не була програма, очищення даних є важливим підготовчим кроком для успішного кластерного аналізу. Кластеризація працює на рівні набору даних, де кожна точка оцінюється відносно інших, тому дані мають бути якомога повнішими.\nКластеризація вимірюється за допомогою внутрішньокластерної та міжкластерної відстані.\n\nВнутрішньокластерна відстань — це відстань між точками даних усередині кластера.\nМіжкластерна відстань — це відстань між точками даних у різних кластерах.\n\n\nТипи R-алгоритмів кластеризації\nІснує понад 100 доступних алгоритмів кластеризації, і всі вони відрізняються один від одного багатьма різними аспектами. Класифікувати ці алгоритми нелегко, але їх можна розділити на чотири категорії.\n1. Моделі зв’язності\nМоделі на основі зв’язності класифікують точки даних у кластери на основі відстані між ними. Основний принцип цих моделей полягає в тому, що об’єкти, які знаходяться ближче один до одного, ймовірно, будуть більш схожими один на одного, ніж об’єкти, які знаходяться далі.\nМоделі зв’язності можуть мати два різних підходи. Перший полягає в тому, щоб розділити всі точки на кластери, а потім об’єднати їх по мірі збільшення відстані. Другий підхід полягає в тому, щоб об’єднати всі точки в один кластер, а потім розділити їх на окремі кластери по мірі збільшення відстані. Міра відстані також може відрізнятися від алгоритму до алгоритму.\n2. Центроїдні моделі\nЦентроїдні моделі - це ітеративні(1) алгоритми кластеризації (в яких поняття подібності виводиться з того, наскільки близька точка даних до центроїда кластера). Тобто їх ідея схожості виводиться з відстані від центроїда кластера. Ці алгоритми вимагають попереднього визначення кількості кластерів. З кожною ітерацією вони коректують положення центроїда кластерів, а також коригують класифікацію кожної точки даних.\n\nІтера́ція - повторне застосування математичної операції (із зміненими даними) при розв’язанні обчислювальних задач, яке дає можливість поступово наблизитися до правильного результату./ Результат багаторазового повторення якоїсь математичної операції.\n\n3. Моделі розподілу\nМоделі розподілу відрізняються від інших типів алгоритмів кластеризації тим, що вони розглядають ймовірність належності точки даних до певного кластеру замість будь-якої міри відстані. Ці моделі часто страждають від надмірного перенавантаження.\n4. Моделі щільності\nМоделі щільності враховують щільність точок у різних частинах простору для створення кластерів у підпросторах з однаковою щільністю. Вони виділяють різні підпростори на основі щільності точок даних, присутніх у них, і призначають точки даних окремим кластерам.\nПакети для кластеризації\n\ncluster - пакет, що містить в собі різні методи для кластеризації даних.\nstats - базовий пакет, що має достатню кліькітсь функцій та методів для кластеризації.\nClusterR.\nclusterSim - пакет, що допомагає підібрати оптимальну процедуру кластеризації для певного датасету."
  },
  {
    "objectID": "03-сlustering.html#практична-частина",
    "href": "03-сlustering.html#практична-частина",
    "title": "2  Clustering: теорія та пакети R",
    "section": "2.3 Практична частина",
    "text": "2.3 Практична частина\nОгляд даних\nДля початку зчитаємо набір даних, з яким ми будемо працювати і глянемо на його структуру. Він містить інформацію про комунальні послуги в різних компаніях.\n\nutilities <- read.csv(\"data/utilities.csv\")\nstr(utilities)\n\n'data.frame':   22 obs. of  9 variables:\n $ Company     : chr  \"Arizona \" \"Boston \" \"Central \" \"Commonwealth\" ...\n $ Fixed_charge: num  1.06 0.89 1.43 1.02 1.49 1.32 1.22 1.1 1.34 1.12 ...\n $ RoR         : num  9.2 10.3 15.4 11.2 8.8 13.5 12.2 9.2 13 12.4 ...\n $ Cost        : int  151 202 113 168 192 111 175 245 168 197 ...\n $ Load        : num  54.4 57.9 53 56 51.2 60 67.6 57 60.4 53 ...\n $ D.Demand    : num  1.6 2.2 3.4 0.3 1 -2.2 2.2 3.3 7.2 2.7 ...\n $ Sales       : int  9077 5088 9212 6423 3300 11127 7642 13082 8406 6455 ...\n $ Nuclear     : num  0 25.3 0 34.3 15.6 22.5 0 0 0 39.2 ...\n $ Fuel_Cost   : num  0.628 1.555 1.058 0.7 2.044 ...\n\n\nПобудуємо схему розсіювання вартості палива(Fuel_Cost) до продажів(Sales).\n\nplot(Fuel_Cost ~ Sales, utilities)\nwith(utilities, text(Fuel_Cost ~ Sales, labels = Company, pos = 4))\n\n\n\n\nБачимо, що в 4 компаніях наявні високі показники продажів, але низькі показники вартості палива. Є компанії всередині, що мають помірні показники вартості палива та продажів. І є компанії з низькими значеннями продажів, але високою вартістю палива. Тобто, ми можемо розділити загальну схему на 3 групи (кластери), які будуть базуватись лише на 2 змінних.\nНормалізація даних\nВарто пам’ятати, що для кластерного аналізу нам потрібні лише числові змінні, тому ми маємо прибрати першу змінну.\n\nnewUtilities <- utilities[,-c(1,1)]\nhead(newUtilities)\n\n\n\nA data.frame: 6 × 8\n\n    Fixed_chargeRoRCostLoadD.DemandSalesNuclearFuel_Cost\n    <dbl><dbl><int><dbl><dbl><int><dbl><dbl>\n\n\n    11.06 9.215154.4 1.6 9077 0.00.628\n    20.8910.320257.9 2.2 508825.31.555\n    31.4315.411353.0 3.4 9212 0.01.058\n    41.0211.216856.0 0.3 642334.30.700\n    51.49 8.819251.2 1.0 330015.62.044\n    61.3213.511160.0-2.21112722.51.241\n\n\n\n\nБачимо, що деякі змінні як Nuclear та Fuel_Cost мають дуже низькі показники на відміну від Sales. Це може зіграти погану роль, адже більші показники будуть домінувати, тому потрібно нормалізувати набір даних, аби середнє значення стало 0, а стандартна похибка приблизно дорівнювала 1.\n\nmeanUtilities <- apply(newUtilities, 2, mean)\nhead(meanUtilities)\nsdUtilities <- apply(newUtilities, 2, sd)\nhead(sdUtilities)\nnewUtilities <- scale(newUtilities, mean, sd)\nhead(newUtilities)\n\nFixed_charge1.11409090909091RoR10.7363636363636Cost168.181818181818Load56.9772727272727D.Demand3.24090909090909Sales8914.04545454545\n\n\nFixed_charge0.184511171191265RoR2.24404937016938Cost41.1913494890283Load4.46114781495075D.Demand3.11825028637613Sales3549.984030511\n\n\nERROR: Error in as.numeric(center): cannot coerce type 'closure' to vector of type 'double'\n\n\nПісля цього ми можемо обрахувати так звану Евклідову відстань (довжину між 2 точками).\n\ndistance <- dist(newUtilities)\nprint(distance, digits = 3)\n\n         1       2       3       4       5       6       7       8       9\n2   3989.4                                                                \n3    140.4  4125.0                                                        \n4   2654.3  1335.5  2789.8                                                \n5   5777.2  1788.1  5912.6  3123.2                                        \n6   2050.5  6039.7  1915.2  4704.4  7827.4                                \n7   1435.3  2554.3  1571.3  1219.6  4342.1  3485.7                        \n8   4006.1  7994.2  3872.3  6659.5  9782.2  1959.7  5440.5                \n9    671.3  3318.3   807.9  1983.3  5106.1  2721.7   764.1  4676.6        \n10  2622.7  1367.1  2758.6    43.6  3155.1  4672.8  1187.9  6627.3  1951.6\n11  8364.0 12353.1  8229.2 11018.1 14141.0  6314.4  9799.0  4359.6  9035.0\n12  2923.1  1066.6  3058.7   271.5  2854.1  4973.5  1488.0  6928.3  2252.0\n13  1899.3  2091.2  2035.4   756.8  3879.2  3949.1   466.6  5903.4  1228.4\n14   598.6  4586.3   461.3  3251.0  6373.7  1454.3  2032.6  3412.3  1269.1\n15  2609.0  1380.7  2744.5    56.6  3168.2  4659.4  1174.1  6614.5  1938.0\n16  6914.7 10903.1  6780.4  9568.4 12691.2  4866.1  8349.4  2909.0  7585.5\n17  3363.1   629.8  3498.1   710.3  2414.7  5413.1  1928.4  7368.8  2692.2\n18  1063.0  5052.3   928.7  3717.2  6840.2   988.0  2498.1  2943.5  1734.1\n19  4430.3  8419.6  4295.0  7084.4 10207.4  2380.1  5865.4   447.8  5101.4\n20  1790.5  2199.7  1925.8   864.3  3987.3  3840.2   358.5  5796.0  1119.9\n21  2427.6  1562.2  2563.6   232.5  3350.1  4478.0   992.5  6432.1  1756.4\n22  1016.6  5005.1   883.5  3670.0  6793.0  1036.0  2451.2  2990.0  1687.2\n        10      11      12      13      14      15      16      17      18\n2                                                                         \n3                                                                         \n4                                                                         \n5                                                                         \n6                                                                         \n7                                                                         \n8                                                                         \n9                                                                         \n10                                                                        \n11 10986.1                                                                \n12   304.3 11287.0                                                        \n13   724.1 10262.2  1026.5                                                \n14  3219.8  7768.4  3520.0  2496.6                                        \n15    53.3 10973.0   314.4   713.7  3205.7                                \n16  9536.2  1452.2  9837.3  8812.3  6319.9  9523.4                        \n17   744.3 11727.1   442.1  1467.0  3959.2   754.6 10277.7                \n18  3685.5  7301.0  3986.1  2961.8   470.2  3672.0  5851.9  4426.0        \n19  7052.7  3934.6  7353.4  6328.9  3834.0  7039.3  2488.4  7793.1  3367.3\n20   833.5 10154.1  1134.1   120.0  2386.9   820.2  8704.7  1573.4  2853.3\n21   199.2 10791.0   496.7   531.5  3025.0   186.4  9341.1   938.5  3490.4\n22  3638.1  7348.0  3939.1  2914.2   428.1  3625.1  5898.6  4379.2    59.3\n        19      20      21\n2                         \n3                         \n4                         \n5                         \n6                         \n7                         \n8                         \n9                         \n10                        \n11                        \n12                        \n13                        \n14                        \n15                        \n16                        \n17                        \n18                        \n19                        \n20  6220.3                \n21  6857.7   640.8        \n22  3414.8  2806.2  3443.2\n\n\nТепер ми бачимо, наскільки далеко знаходиться одна точка відносно інших. Наприклад, 11 ряд 5 колонка - 6.46. Це означає, що 5 компанія та 11 компанія мають дуже різні показники і, можливо, входять в різні групи. Тобто, між ними мало схожого.\n\nІєрархічне кастеризування за допомогою дендограми\n\nhcluster <- hclust(distance) \nplot(hcluster)\n\nТак як це ієрархічне кастеризування, то кожна компанія являє собою окремий кластер і тепер ми можемо знайти, яка є ближчою до іншої. Наприклад, компанія 10 та 13 стоять поряд. Це означає, що вони можуть бути згруповані в один кластер. Так буде продовжувати до моменту, поки всі компанії не поєднаються в один кластер. Чітко можна розгледіти 3 кластери: від 3 компанії до 20 компанії; від 11 компанії до 16 компанії; від 17 компанії до 21 компанії. Змінимо числа на назви компаній.\n\nplot(hcluster, labels = utilities$Company)\n\nТепер можемо глянути на те, скільки компаній входять в який кластер. Для прикладу, все ж зупинимось на теорії про існування 3 кластерів у цьому датасеті.\n\nmembers <- cutree(hcluster, 3)\ntable(members)\n\nБачимо, що в перший кластер входить 14 компаній, в другий - 5, в третій - 3. Тепер використаємо пакет cluster, аби візуально відобразити структуру кластерів і те, наскільки кожна змінна описує цей кластер. Бачимо, що є від’ємні значення в кластері. З цього можемо зробити висновок про те, що це можуть бути аномальні показники.\n\n\n#install.packages(\"cluster\")\nlibrary(cluster)\n\n\nplot(silhouette(cutree(hcluster, 3), distance))\n\n\nK-means Clustering\nЗа допомогою функції kmeans() ми можемо побудувати бажану кількість кластерів.\n\nkMeanCluster <- kmeans(newUtilities, 3)\nkMeanCluster\n\nВ цьому варіанті маємо в першому кластері 7 компаній, в другій - 3, в третій - 12.\n\nplot(Fuel_Cost ~ Sales, utilities, col = kMeanCluster$cluster)\n\n\nКластеризація моделями щільності\n\n# install.packages(\"factoextra\")\nlibrary(factoextra)\n\n\nfviz_cluster(kMeanCluster, data = newUtilities)"
  }
]